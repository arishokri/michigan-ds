{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7539457c865cc3aec382650368b3be67",
     "grade": false,
     "grade_id": "cell-2ccb6b5d27ca3b7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 4\n",
    "\n",
    "Before working on this assignment please read these instructions fully. In the submission area, you will notice that you can click the link to **Preview the Grading** for each step of the assignment. This is the criteria that will be used for peer grading. Please familiarize yourself with the criteria before beginning the assignment.\n",
    "\n",
    "This assignment requires that you find **at least two datasets** on the web which are related, and that you visualize these datasets to answer the assignment question. You are free to utilize datasets with any location or domain, the usage of **Ann Arbor sports and athletics** datasets in the example is just a suggestion.\n",
    "\n",
    "You are welcome to choose datasets at your discretion, but keep in mind **they will be shared with your peers**, so choose appropriate datasets. Sensitive, confidential, illicit, and proprietary materials are not good choices for datasets for this assignment. You are welcome to upload datasets of your own as well, and link to them using a third party repository such as github, pastebin, etc. Please be aware of the Coursera terms of service with respect to intellectual property.\n",
    "\n",
    "Also, you are welcome to preserve data in its original language, but for the purposes of grading you should provide english translations. You are welcome to provide multiple visuals in different languages if you would like!\n",
    "\n",
    "As this assignment is for the whole course, you must incorporate principles discussed in the first week, such as having as high data-ink ratio (Tufte) and aligning with Cairoâ€™s principles of truth, beauty, function, and insight.\n",
    "\n",
    "Here are the assignment instructions:\n",
    "\n",
    " * You must state a question you are seeking to answer with your visualizations.\n",
    " * You must provide at least two links to available datasets. These could be links to files such as CSV or Excel files, or links to websites which might have data in tabular form, such as Wikipedia pages.\n",
    " * You must upload an image which addresses the research question you stated. In addition to addressing the question, this visual should follow Cairo's principles of truthfulness, functionality, beauty, and insightfulness.\n",
    " * You must contribute a short (1-2 paragraph) written justification of how your visualization addresses your stated research question.\n",
    "\n",
    "## Tips\n",
    "* Wikipedia is an excellent source of data, and I strongly encourage you to explore it for new data sources.\n",
    "* Many governments run open data initiatives at the city, region, and country levels, and these are wonderful resources for localized data sources.\n",
    "* Several international agencies, such as the [United Nations](http://data.un.org/), the [World Bank](http://data.worldbank.org/), the [Global Open Data Index](http://index.okfn.org/place/) are other great places to look for data.\n",
    "* This assignment requires you to convert and clean datafiles. Check out the discussion forums for tips on how to do this from various sources, and share your successes with your fellow students!\n",
    "\n",
    "## Example\n",
    "Looking for an example? Here's what our course assistant put together as an example! [Example Solution File](./readonly/Assignment4_example.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chicago Weather and Crime Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching Crime Data from City of Chicago Data Portal's Socrata Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from sodapy import Socrata\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "\n",
    "def write_results(data: List[dict], filepath: str, number: int) -> str:\n",
    "    \"\"\"\n",
    "    Function used internally by other functions to write csv files.\n",
    "    \"\"\"\n",
    "    base_path = Path(filepath)\n",
    "    new_filename = base_path.stem + \"_\" + str(number) + base_path.suffix\n",
    "    new_filepath = base_path.parent / new_filename\n",
    "    with open(new_filepath, mode=\"w\", encoding=\"utf-8\") as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(data[10].keys())\n",
    "        writer.writerows([item.values() for item in data])\n",
    "    return new_filepath\n",
    "\n",
    "\n",
    "def save_data_as_csv(\n",
    "    dataset_id: str,\n",
    "    app_token: str,\n",
    "    limit: int = 10**5,\n",
    "    offset: int = 0,\n",
    "    sleep_t: int = 15,\n",
    "    filepath: str = \"fetched_data/save.csv\",\n",
    "    columns: Optional[List[str]] = None,\n",
    ") -> List[list]:\n",
    "    \"\"\"\n",
    "    Fetch data from the City of Chicago API and saves it in batches.\n",
    "\n",
    "    Args:\n",
    "        dataset_id (str): The dataset identifier.\n",
    "        app_token (str): The application token for API access.\n",
    "        limit (int): Number of records per page (default: 1000,000).\n",
    "        offset (int): Will fetch data starting from the defined offset index (default is 0).\n",
    "        sleep_t (int): How long (in seconds) to sleep between page requests.\n",
    "        filepath (str): Path to the file, will be appended with offset point number.\n",
    "        columns (Optional[List[str]]): A list of columns to be included in data.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of JSON objects containing the crime data.\n",
    "    \"\"\"\n",
    "    domain = \"data.cityofchicago.org\"\n",
    "    client = Socrata(domain=domain, app_token=app_token)\n",
    "\n",
    "    if columns:\n",
    "        columns = \", \".join([col for col in columns])\n",
    "\n",
    "    temp_data = []\n",
    "    all_data = []\n",
    "    break_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.get(\n",
    "                dataset_identifier=dataset_id,\n",
    "                limit=limit,\n",
    "                offset=offset,\n",
    "                content_type=\"json\",\n",
    "                select=columns\n",
    "            )\n",
    "            temp_data.extend(response)\n",
    "            offset = offset + limit\n",
    "            break_counter += 1\n",
    "            print(f\"The first {offset} entries successfully fetched.\")\n",
    "            if break_counter >= 10:\n",
    "                # Write data if more than 10 requests run.\n",
    "                new_filepath = write_results(\n",
    "                    data=temp_data, filepath=filepath, number=offset\n",
    "                )\n",
    "                print(\n",
    "                    f\"Total of {offset:,} entries were fetched. Results saved in {new_filepath}\"\n",
    "                )\n",
    "                all_data.extend(temp_data)\n",
    "                temp_data = []\n",
    "                break_counter = 0\n",
    "            sleep(sleep_t)\n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            print(f\"Read timeout occured, will try after {sleep_t} seconds.\")\n",
    "            sleep(sleep_t)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            if temp_data:\n",
    "                new_filepath = write_results(\n",
    "                    data=temp_data, filepath=filepath, number=offset\n",
    "                )\n",
    "                print(f\"Some fetched data saved in {new_filepath}.\\n{e}\")\n",
    "            print(f\"Error occured {e}\")\n",
    "            break\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query and Save Crime Data\n",
    "\n",
    "To query and save crime data you can start with setting the `first_page` parameter to 0 (or leave it blank), pick a reasonable `n_pages` parameter (e.g. query through 500 pages takes about 25 mins). Run and save the results recursively each time starting at the previous `first_page + n_pages` until there is no more results.\n",
    "\n",
    "*To query all data without using any filters takes a significant amount of time (north of 4 hours)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sodapy import Socrata\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "starting_offset = 6*(10**6)\n",
    "\n",
    "load_dotenv()\n",
    "app_token = os.getenv(\"APP_TOKEN\")\n",
    "dataset_id = \"ijzp-q8t2\"\n",
    "columns = [\n",
    "    \"id\",\n",
    "    \"case_number\",\n",
    "    \"date\",\n",
    "    \"block\",\n",
    "    \"iucr\",\n",
    "    \"primary_type\",\n",
    "    \"description\",\n",
    "    \"location_description\",\n",
    "    \"arrest\",\n",
    "    \"domestic\",\n",
    "    \"beat\",\n",
    "    \"district\",\n",
    "    \"ward\",\n",
    "    \"community_area\",\n",
    "    \"fbi_code\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "]\n",
    "\n",
    "results = save_data_as_csv(\n",
    "    dataset_id=dataset_id,\n",
    "    app_token=app_token,\n",
    "    limit=10**5,\n",
    "    offset=starting_offset,\n",
    "    filepath=\"data/crimes.csv\",\n",
    "    columns=columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results))\n",
    "results[10]"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mooc_adswpy_v1_assignment4"
   ]
  },
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
