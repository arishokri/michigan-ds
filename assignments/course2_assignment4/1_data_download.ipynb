{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Crime Data\n",
    "\n",
    "#### Functions for Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from sodapy import Socrata\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "\n",
    "def write_results(data: List[dict], filepath: str, number: int) -> str:\n",
    "    \"\"\"\n",
    "    Function used internally by other functions to write csv files.\n",
    "    \"\"\"\n",
    "    base_path = Path(filepath)\n",
    "    new_filename = base_path.stem + \"_\" + f\"{number:,}\" + base_path.suffix\n",
    "    new_filepath = base_path.parent / new_filename\n",
    "    with open(new_filepath, mode=\"w\", encoding=\"utf-8\") as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(data[10].keys())\n",
    "        writer.writerows([item.values() for item in data])\n",
    "    return new_filepath\n",
    "\n",
    "\n",
    "def save_data_as_csv(\n",
    "    dataset_id: str,\n",
    "    app_token: str,\n",
    "    limit: int = 10**5,\n",
    "    offset: int = 0,\n",
    "    sleep_t: int = 15,\n",
    "    filepath: str = \"fetched_data/save.csv\",\n",
    "    columns: Optional[List[str]] = None,\n",
    ") -> List[list]:\n",
    "    \"\"\"\n",
    "    Fetch data from the City of Chicago API and saves it in batches.\n",
    "\n",
    "    Args:\n",
    "        dataset_id (str): The dataset identifier.\n",
    "        app_token (str): The application token for API access.\n",
    "        limit (int): Number of records per page (default: 1000,000).\n",
    "        offset (int): Will fetch data starting from the defined offset index (default is 0).\n",
    "        sleep_t (int): How long (in seconds) to sleep between page requests.\n",
    "        filepath (str): Path to the file, will be appended with offset point number.\n",
    "        columns (Optional[List[str]]): A list of columns to be included in data.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of JSON objects containing the crime data.\n",
    "    \"\"\"\n",
    "    domain = \"data.cityofchicago.org\"\n",
    "    client = Socrata(domain=domain, app_token=app_token)\n",
    "\n",
    "    if columns:\n",
    "        columns = \", \".join([col for col in columns])\n",
    "\n",
    "    temp_data = []\n",
    "    all_data = []\n",
    "    break_counter = 0\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.get(\n",
    "                dataset_identifier=dataset_id,\n",
    "                limit=limit,\n",
    "                offset=offset,\n",
    "                content_type=\"json\",\n",
    "                select=columns,\n",
    "            )\n",
    "            if not response:\n",
    "                print(\"No new data returned.\")\n",
    "                break\n",
    "            temp_data.extend(response)\n",
    "            offset = offset + limit\n",
    "            break_counter += 1\n",
    "            print(f\"The first {offset} entries successfully fetched.\")\n",
    "            if break_counter >= 10:\n",
    "                # Write data if more than 10 requests run.\n",
    "                new_filepath = write_results(\n",
    "                    data=temp_data, filepath=filepath, number=offset\n",
    "                )\n",
    "                print(\n",
    "                    f\"Total of {offset:,} entries were fetched. Results saved in {new_filepath}\"\n",
    "                )\n",
    "                all_data.extend(temp_data)\n",
    "                temp_data = []\n",
    "                break_counter = 0\n",
    "            sleep(sleep_t)\n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            print(f\"Read timeout occured, will try after {sleep_t} seconds.\")\n",
    "            sleep(sleep_t)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            if temp_data:\n",
    "                new_filepath = write_results(\n",
    "                    data=temp_data, filepath=filepath, number=offset\n",
    "                )\n",
    "                print(f\"Some fetched data saved in {new_filepath}.\\n{e}\")\n",
    "            print(f\"Error occured {e}\")\n",
    "            break\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query and Save Crime Data from Socrata Platform\n",
    "\n",
    "To query and save crime data you can start with setting the `first_page` parameter to 0 (or leave it blank), pick a reasonable `n_pages` parameter (e.g. query through 500 pages takes about 25 mins). Run and save the results recursively each time starting at the previous `first_page + n_pages` until there is no more results.\n",
    "\n",
    "*To query all data without using any filters takes a significant amount of time (north of 4 hours)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "starting_offset = 9*10**6\n",
    "\n",
    "load_dotenv()\n",
    "app_token = os.getenv(\"APP_TOKEN\")\n",
    "dataset_id = \"ijzp-q8t2\"\n",
    "columns = [\n",
    "    \"id\",\n",
    "    \"case_number\",\n",
    "    \"date\",\n",
    "    \"block\",\n",
    "    \"iucr\",\n",
    "    \"primary_type\",\n",
    "    \"description\",\n",
    "    \"location_description\",\n",
    "    \"arrest\",\n",
    "    \"domestic\",\n",
    "    \"beat\",\n",
    "    \"district\",\n",
    "    \"ward\",\n",
    "    \"community_area\",\n",
    "    \"fbi_code\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "]\n",
    "\n",
    "results = save_data_as_csv(\n",
    "    dataset_id=dataset_id,\n",
    "    app_token=app_token,\n",
    "    limit=5*10**3,\n",
    "    offset=starting_offset,\n",
    "    filepath=\"data/crimes.csv\",\n",
    "    columns=columns,\n",
    "    sleep_t = 5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Data\n",
    "\n",
    "Combining data that were saved into scattered CSV files as a single CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Reading all data files\n",
    "data = []\n",
    "for filename in os.listdir(\"data/\"):\n",
    "    if filename.startswith(\"crimes_\"):\n",
    "        path = os.path.join(\"data/\", filename)\n",
    "        with open(path, \"r\") as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            for line in reader:\n",
    "                data.append(line)\n",
    "\n",
    "# Removing duplicated header rows.\n",
    "data_cleaned = []\n",
    "for i, row in enumerate(data):\n",
    "    if i == 0 or row[0] != \"id\":\n",
    "        data_cleaned.append(row)\n",
    "\n",
    "# Writing data\n",
    "with open(\"data/crimes_combined.csv\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerows(data_cleaned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
